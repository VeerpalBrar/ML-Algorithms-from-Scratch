{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.047419</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.091642</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.048707</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.109919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.109413</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.140081</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041378</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.047390</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.109919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071716</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040786</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.047419</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071716</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.047146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Survived  Pclass      Name  Sex       Age  SibSp  Parch  \\\n",
       "0           0         0       3  0.047419    0 -0.091642      1      0   \n",
       "1           1         1       1  0.109919    1  0.109413      1      0   \n",
       "2           2         1       3 -0.140081    1 -0.041378      0      0   \n",
       "3           3         1       1  0.109919    1  0.071716      1      0   \n",
       "4           4         0       3  0.047419    0  0.071716      0      0   \n",
       "\n",
       "       Fare  Embarked  \n",
       "0 -0.048707         2  \n",
       "1  0.076277         0  \n",
       "2 -0.047390         2  \n",
       "3  0.040786         2  \n",
       "4 -0.047146         2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "\n",
    "#get training data\n",
    "data_train = pd.read_csv('features.csv') #this is preprocessed data\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train.insert(0, 'Ones', 1) # add column of ones for matrix multiplication later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=data_train.sample(frac=0.8,random_state=200)\n",
    "cross=data_train.drop(train.index)\n",
    "# make training data in to matrix\n",
    "y_t = train['Survived']\n",
    "X_t = train.drop(\"Survived\",axis=1)\n",
    "\n",
    "Y = y_t.values\n",
    "X_temp = X_t.values\n",
    "m, n = X_temp.shape\n",
    "X=np.ones((m,n+1))\n",
    "\n",
    "X[:,-n:] = X_temp\n",
    "Y.shape = (m, 1)\n",
    "\n",
    "# make cross validation data into matrix\n",
    "crossy_t = cross['Survived']\n",
    "crossX_t = cross.drop(\"Survived\",axis=1)\n",
    "\n",
    "crossY = crossy_t.values\n",
    "crossX_temp = crossX_t.values\n",
    "crossm, crossn = crossX_temp.shape\n",
    "crossX=np.ones((crossm,crossn+1))\n",
    "\n",
    "crossX[:,-crossn:] = crossX_temp\n",
    "crossY.shape = (crossm, 1)\n",
    "\n",
    "#initalize theta\n",
    "theta = np.zeros(shape=(n+1, 1))\n",
    "lambd=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic funcition we want to have values between 0 and 1. For this the hypothesis is:\n",
    "![h(x) = g(theta*x)](hypothesis.png)\n",
    "\n",
    "Where:\n",
    "![g(z) = 1 over 1 + e to the negative z](sigmoid.png)\n",
    "\n",
    "The hypothesis uses the sigmoid function to return a value between 1 or 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute sigmoid function\n",
    "def sigmoid(z):\n",
    "     return 1.0 / (1.0 + np.e ** (-1.0 * z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compute a cost function. What the cost function does is determine the error between the predicted value and the actual value. But minimizing the cost function, we minmize the error (aka we will be getting better perdictions!)\n",
    "\n",
    "The cost function is: \n",
    "![cost-function](cost-function.png)\n",
    "\n",
    "where the superscript i represents the ith example, and m is the total number of examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "def cost_function(theta, X, Y):\n",
    "    '''Determine the cost. \n",
    "    theta is an array holding parameters.\n",
    "    X is the input features. \n",
    "    Y is the actual classification. '''\n",
    "    hypothesis =sigmoid(X.dot(theta).T)\n",
    "    first_half =-Y.T.dot(np.log(hypothesis.T))\n",
    "    second_half = (1-Y).T.dot(np.log(1-hypothesis.T))\n",
    "\n",
    "    overall_cost = (1.0/m)*sum(first_half - second_half)\n",
    "    #this is a vectorized implementation of the cost function\n",
    "    #therefore 'overall_cost' is a single element matrix with the total cost\n",
    "    \n",
    "    #this next line just converts it to an integer value\n",
    "    #print(overall_cost)\n",
    "    return np.sum(overall_cost) \n",
    "print(cost_function(theta, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function calculate the error of a given theta. We want to minimize the cost (ie error) as much as possible and find the best possible theta parameters. \n",
    "\n",
    "Therefore, for each parameter, we calculate the gradient and then adjust the parameter by that gradient. \n",
    "\n",
    "NOTE: Remember that the cost function is a convex function and we are looking for the local minimum. The gradient always points in the direction of greatest change, and so by finding and moving is the direction of the gradient, we move closer to the minimum. \n",
    "\n",
    "To calculate the gradient for parameter j, we use:\n",
    "![gradient](gradient.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient(theta, X, Y):\n",
    "    hypothesis =sigmoid(X.dot(theta).T)\n",
    "    error = (hypothesis - Y)\n",
    "    grad = np.zeros(X.shape[1])\n",
    "    \n",
    "    \n",
    "    #calculate the gradient for each parameter\n",
    "    for i in range(X.shape[1]):\n",
    "        grad[i] = (1.0/m)*sum((error).T.dot(X[:,i]))\n",
    "        \n",
    "    #print(grad)\n",
    "    return np.matrix(grad)\n",
    "#gradient(theta, X, Y, lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code does not execute gradient decent. Rather it executes one step of gradient decent. \n",
    "\n",
    "To do gradient decent, we will use an optimization function from the scipy library which will repeatedly call the cost function, trying to minimize it, while using a gradient function to determine how to minimize cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(theta, X, Y):\n",
    "    Result = opt.minimize(fun = cost_function, x0 = theta, args = (X, Y), method = 'TNC');\n",
    "    optimal_theta = Result.x;\n",
    "    return optimal_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The last thing remaining is to predict survivors from test data. We create a function which uses the optimized theta with test features and see if it predicts the outcome correctly. We will say that when the sigmoid function returns a value greater than 0.5, it is predicting survival. Otherwise, it predicts the person did not survive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    # Predicts the survival (0 or 1) based on learned logistic regression data\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    pred = np.zeros(shape=(h.shape[0], 1))\n",
    "    for i in range(0, h.shape[0]):\n",
    "        if h[i] > 0.5:\n",
    "            pred[i] = 1\n",
    "        else:\n",
    "            pred[i] = 0\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets prepare the test data matricies and begin predicting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  80.5049088359\n",
      "cross accuracy:  76.9662921348\n"
     ]
    }
   ],
   "source": [
    "optimal_theta = train(theta, X, Y, )\n",
    "p = predict(np.array(optimal_theta), X)\n",
    "t = (p == Y)\n",
    "print (\"test accuracy: \",np.mean(t)*100)\n",
    "\n",
    "y_test = predict(np.array(optimal_theta), crossX) \n",
    "crosst = (y_test == crossY)\n",
    "\n",
    "print (\"cross accuracy: \", np.mean(crosst)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use learning curves to showcase how well logistic regression performs on both the training set and the cross validation set as you add more examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerpal\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Veerpal\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:628: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]\n",
      "C:\\Users\\Veerpal\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: RuntimeWarning: overflow encountered in power\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Veerpal\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEdCAYAAAAfA1CsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncqXOplmBPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c79fba2518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def learningcurve(X, Y, crossX, crossY):\n",
    "    error_train = [0]*19;\n",
    "    error_val = [0]*19;\n",
    "    value = 0\n",
    "    for i in range(5, 100 , 5):\n",
    "        x1 = X[0:i, :]\n",
    "        y1 = Y[0:i,:]\n",
    "        \n",
    "        #initalize theta\n",
    "        m, n = x1.shape\n",
    "        theta = np.zeros(shape=(n, 1))\n",
    "        theta = train(theta, x1, y1)\n",
    "        \n",
    "        \n",
    "        error_train[value] = cost_function(theta,x1, y1)\n",
    "        error_val[value] = cost_function(theta, crossX, crossY)\n",
    "        value+=1 \n",
    "\n",
    "    return error_train, error_val\n",
    "train_curve, cross_curve = learningcurve(X, Y, crossX, crossY)\n",
    "plt.plot(train_curve, label=\"training\")\n",
    "plt.plot(cross_curve, label=\"cross validation\")\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compare the logistic regression from scratch algorithm with the built in sklean function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[  3.89698917e-01   3.89698917e-01   1.30949474e-04  -9.85316969e-01\n",
      "   -5.76591913e-01   2.56187102e+00  -1.86641383e+00  -3.25808497e-01\n",
      "    2.05420935e-02   5.44655008e-01  -2.17683461e-01]]\n",
      "Mean squared error: 0.46\n",
      "Variance score: 0.77\n",
      "cross accuracy: 76.9662921348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerpal\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# Create linear regression object\n",
    "regr = linear_model.LogisticRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "regr.fit(X, Y)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % np.mean((regr.predict(crossX) - crossY) ** 2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(crossX, crossY))\n",
    "p = np.matrix(regr.predict(crossX))\n",
    "t = (p == crossY.T)\n",
    "print (\"cross accuracy:\", np.mean(t)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accurage score is the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
